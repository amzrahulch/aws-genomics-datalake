[
{
	"uri": "/en/",
	"title": "Workshop: How to build a Genomics Datalake",
	"tags": [],
	"description": "",
	"content": "Even as data is exploding from various population genomics projects all over the world, researchers in the field find it hard to perform large scale genomics analyses with data scattered across multiple data silos within their organizations. In order to be able to perform more sophisticated tertiary analysis on genomics and clinical data, researchers need to be able to access, aggregate and query the data from a centralized data store in a secure and compliant manner.\nIn this workshop, we will demonstrate how to transform genomic variant data using Amazon EMR to populate a genomics data lake that can then be queried using Amazon Athena. We use Hail (https://hail.is/) a genomics software tool from the Broad Institute, on Amazon EMR to perform the data transformation. We will also demonstrate optimizations that improve query performance, and will use the 1000 genomes data available on the AWS Registry of Open data as an example dataset.\n"
},
{
	"uri": "/en/basics.html",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will work through a solution to help you build a genomics datalake efficiently and cost effectively. This solution allows geneticists and researchers to run analysis easily using common SQL language.\n\r "
},
{
	"uri": "/en/basics/requirements.html",
	"title": "Requirements",
	"tags": [],
	"description": "",
	"content": "To simplify the setup, you can follow this AWS Quick Start solution, Hail on AWS to setup Hail on EMR. In this workshop, we provide a brief instruction to set it manually to help you understand the key dependency.\nYou need to have an AWS account, a VPC with a public subnet already setup. As well as permission to access the following services\n EMR Glue Athena S3  "
},
{
	"uri": "/en/etl.html",
	"title": "VCF conversion",
	"tags": [],
	"description": "",
	"content": "Launch pre processing step using cloudformation We will use cloudformation template to deploy EMR cluster which has Hail compiled and installed already. The cloudformation template takes inputs as shown below.\n\r Replace the value for output S3 path and EMR Logs to a path in your own account. While chosing a VPC pick a public subnet or a private subnet with a NAT gateway for outbound internet traffic.\n\rThe optional values here can be left unchanged.\n\r Once you hit create stack it takes a few minutes for the EMR cluster to appear as Starting.\n\r Within a few minutes (5-10mins) the VCFs in the input path provided will be processed and written as parquet to the output path. The EMR cluster will be auto terminated after this processing step completes.\n\r How hail is used to transform VCF to Parquet The script that was used to transform is located here: s3://redshift-demos/genomics/vcfToParquetTransform.py This script takes two input parameters: vcf_s3_location output_location. You can provide your own custom script in the cloudformation template above using the HailScriptPath parameter.\nFor a single VCF file, the job takes 1-2 minutes. Larger VCF could take longer time.\n"
},
{
	"uri": "/en/analysis.html",
	"title": "Analysis via Athena",
	"tags": [],
	"description": "",
	"content": "During this section we will be covering the following topics:\n Querying the 1000 genomes transformed dataset in RODA (https://registry.opendata.aws/ilmn-dragen-1kgp/). The dataset is under s3://aws-roda-hcls-datalake/thousandgenomes_dragen/ Querying the transformed VCF files from the previous EMR processing step How to manage new samples  "
},
{
	"uri": "/en/analysis/newsamples.html",
	"title": "Manage new samples",
	"tags": [],
	"description": "",
	"content": "For clinvar data, the dataset is refreshed whenever the source data is updated. For variants data, sequenced samples are usually not updated, but new samples will be added over time. How to integrate the new samples to our dataset so our analysis can get up-to-date result?\nOnce new vcf files are uploaded to S3, we can run the hail job to convert them to parquet. then add them to various optimized data copies.\ninsert into var_part_by_chrom select * from var_part_by_sample where partition_0 in (\u0026lt;list of new samples\u0026gt;); \rNote that for the nested dataset, we need to re-built it to have new samples nested properly with variant sites.\n\rFirst drop the table and delete underlying S3 data, then re-run the CTAS query. Below query demonstrate if you only want to update data of a specific chromosome. Note you need to change the s3 bucket to one of your own.\ncreate table var_nested_chr21 with ( external_location = \u0026#39;s3://genomics_datalake/var_nested/chrom=chr21/\u0026#39;, format = \u0026#39;ORC\u0026#39; ) as select variant_id, pos, ref, alt, array_agg(CAST(ROW(sample_id, \u0026#34;gt.alleles\u0026#34;) AS ROW(id VARCHAR, gts ARRAY(INTEGER)))) as samples from var_part_by_chrom where chrom = \u0026#39;chr21\u0026#39; group by 1,2,3,4 "
},
{
	"uri": "/en/analysis/thousandgenomesnew.html",
	"title": "Querying 1000 Genomes data",
	"tags": [],
	"description": "",
	"content": "We will use the parquet/ORC transformed variant data from the 3502 DRAGEN-reanalyzed 1000 Genomes dataset available in 3 different schemas at s3://aws-roda-hcls-datalake/thousandgenomes_dragen/:\nvar_partby_samples - Dataset partitioned by sample ID\nvar_partby_chrom - Dataset partitioned by chromosome and bucketed by samples\nvar_nested - Nested schema consisting of variant sites with sample IDs and genotypes that contain the variant\nWe use the transformed annotations from ClinVar that are available at https://registry.opendata.aws/clinvar/ and the transformed population allele frequency data from gnomAD at to demonstrate how to make queries that use the raw variant data with annotations.\nPlease use the CloudFormation templates for 1000 Genomes DRAGEN, ClinVar and gnomAD available at https://github.com/aws-samples/data-lake-as-code/tree/roda to add these tables to your Glue Data Catalog.\nWe use Clinvar as Annotation dataset. The dataset is publically available in RODA, you can create an external table in Athena (or use the cloudformation template from above) and start accessing it immediately.\n-- Clinvar dataset CREATE EXTERNAL TABLE `variant_summary`( `alleleid` bigint, `type` string, `name` string, `geneid` bigint, `genesymbol` string, `hgnc_id` string, `clinicalsignificance` string, `clinsigsimple` bigint, `lastevaluated` string, `rcvaccession` string, `phenotypeids` string, `phenotypelist` string, `origin` string, `originsimple` string, `assembly` string, `chromosomeaccession` string, `chromosome` string, `start` bigint, `stop` bigint, `referenceallele` string, `alternateallele` string, `cytogenetic` string, `reviewstatus` string, `numbersubmitters` bigint, `guidelines` string, `testedingtr` string, `otherids` string, `submittercategories` bigint, `variationid` bigint, `positionvcf` bigint, `referenceallelevcf` string, `alternateallelevcf` string, `rsid_dbsnp` bigint, `nsv_esv_dbvar` string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION\u0026#39;s3://aws-roda-hcls-datalake/clinvar_summary_variants/variant_summary/\u0026#39; ; For this workshop, we have the dataset converted already, you can just create an external table point to it.\nCREATE EXTERNAL TABLE `var_nested`( `variant_id` string COMMENT \u0026#39;\u0026#39;, `pos` int COMMENT \u0026#39;\u0026#39;, `ref` string COMMENT \u0026#39;\u0026#39;, `alt` string COMMENT \u0026#39;\u0026#39;, `samples` array\u0026lt;struct\u0026lt;id:string,gts:array\u0026lt;int\u0026gt;\u0026gt;\u0026gt; COMMENT \u0026#39;\u0026#39;) PARTITIONED BY ( `chrom` string COMMENT \u0026#39;\u0026#39;) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\u0026#39; LOCATION \u0026#39;s3://aws-roda-hcls-datalake/thousandgenomes_dragen/var_nested/\u0026#39; ; -- Discover partitions MSCK REPAIR TABLE var_nested; For the end user who is a researcher performing analyses at a cohort level, the typical types of queries are to identify samples that meet a specific criteria. For example, samples that have variants in a specific gene or region, samples that have a pathogenic or likely pathogenic variant in a specific gene, samples that have rare (minor allele frequency \u0026lt; 0.01) variants in a specific region. The typical mode of operation would be to interactively query using progressively narrower filters to identify a cohort of an appropriate size, and then fetch all the details about the variants and samples themselves. Here is an example set of queries where a researcher is looking for samples that have VUS (variants of uncertain significance) variants in the BRCA1 gene, which is present on chromosome 17 between the loci, 43,044,295 and 43,125,364. We will first query to see how many samples have variants in the BRCA1 gene:\nselect count(DISTINCT(sample.id)) from ( select samples from var_nested where chrom=\u0026#39;chr17\u0026#39; and pos between 43044295 and 43125364 ) as f, unnest(f.samples) as s(sample); This query returns 3202, which means all samples meet the criteria. We need to define a narrower filter to identify samples of interest. This will involve using the clinical significance annotations from ClinVar variant summary table. First, in order to make the queries simpler, we can define a “view” that creates a “variant_id”, consisting of the chromosome, position, ref and alt fields (can be used to join with the variant table), and includes only the fields of interest to us.\n-- View to cleanup and build variant_id that will be used to join with variant data. create or replace view clinvar as select concat(\u0026#39;chr\u0026#39;, chromosome, \u0026#39;:\u0026#39;, referenceallelevcf, \u0026#39;:\u0026#39;, alternateallelevcf, \u0026#39;:\u0026#39;, cast(positionvcf as varchar)) as variant_id , chromosome as chrom , positionvcf as pos , referenceallelevcf as ref , alternateallelevcf as alt , genesymbol as genename , clinicalsignificance as clinvar_clnsig , split(phenotypeids, \u0026#39;,\u0026#39;) as phenotypeids from variant_summary where assembly = \u0026#39;GRCh38\u0026#39; and referenceallelevcf \u0026lt;\u0026gt; \u0026#39;na\u0026#39; and alternateallelevcf \u0026lt;\u0026gt; \u0026#39;na\u0026#39; ; We can then use this “clinvar” view in subsequent queries, such as to further filter the original query by searching for only those samples that have variants of Uncertain Significance.\nselect count(DISTINCT(sample.id (http://sample.id/))) from ( select samples from var_nested as v join clinvar a on v.variant_id = a.variant_id where *a**.clinvar_clnsig = \u0026#39;Uncertain significance\u0026#39;* and v.chrom=\u0026#39;chr17\u0026#39; and v.pos between 43044295 and 43125364 ) as f, unnest(f.samples) as s(sample); This query returns a result of 1550 samples, which has reduced our sample set by approximately 50%. If we want further confidence that these are indeed Variants of Unknown Significance, we can further filter down to only those samples that have variants with more than one submitter.\nselect count(DISTINCT(sample.id)) from ( select samples from var_nested as v join clinvar a on v.variant_id = a.variant_id where a.clinvar_clnsig = \u0026#39;Uncertain significance\u0026#39; and v.chrom=\u0026#39;chr17\u0026#39; and v.pos between 43044295 and 43125364 *and a.num_submitters \u0026gt; 1* ) as f, unnest(f.samples) as s(sample); This query returns a result set of 41 samples, which can then be investigated further. For more example queries and scenarios, we have created a sample notebook here you can try.\n"
},
{
	"uri": "/en/analysis/queryemrprocessed.html",
	"title": "Querying the transformed VCF files from the previous EMR processing step",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s check if the VCF files have been transformed. You can examine the S3 output location if parquet files are generated:\nThe data is ready to be queried by Athena. We just need to create table schema for it. you can use Glue crawler to detect schema for you.\nCREATE EXTERNAL TABLE `var_part_by_sample`( `variant_id` string, `chrom` string, `pos` int, `alleles` array\u0026lt;string\u0026gt;, `rsid` string, `qual` double, `filters` array\u0026lt;string\u0026gt;, `info.ac` array\u0026lt;int\u0026gt;, `info.af` array\u0026lt;double\u0026gt;, `info.an` int, `info.db` boolean, `info.dp` int, `info.end` int, `info.fs` double, `info.fractioninformativereads` double, `info.lod` double, `info.mq` double, `info.mqranksum` double, `info.qd` double, `info.r2_5p_bias` double, `info.readposranksum` double, `info.sor` double, `ad` array\u0026lt;int\u0026gt;, `af` array\u0026lt;double\u0026gt;, `dp` int, `fir2` array\u0026lt;int\u0026gt;, `f2r1` array\u0026lt;int\u0026gt;, `gp` array\u0026lt;double\u0026gt;, `gq` int, `gt.alleles` array\u0026lt;int\u0026gt;, `gt.phased` boolean, `mb` array\u0026lt;int\u0026gt;, `pl` array\u0026lt;int\u0026gt;, `pri` array\u0026lt;double\u0026gt;, `ps` int, `sb` array\u0026lt;int\u0026gt;, `sq` double, `sample_id` string, `ref` string, `alt` string) PARTITIONED BY ( `partition_0` string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;s3://aws-roda-hcls-datalake/thousandgenomes_dragen/var_partby_samples/\u0026#39; ; For certain analysis, you may care mostly just variants, not the details of samples. To further optimize query performance for such analysis, we could nested sample data into variants using below query. Note you need to change the s3 bucket to one of your own. This can greatly reduce the record count and speedup analysis. In this data model, the record count is reduced from ~16.9 billion to ~160 million.\ncreate table var_nested with ( external_location = \u0026#39;s3://genomics_datalake/var_nested/\u0026#39;, format = \u0026#39;ORC\u0026#39;, partitioned_by = array[\u0026#39;chrom\u0026#39;], bucketed_by = ARRAY[\u0026#39;pos\u0026#39;], bucket_count = 60, ) as select variant_id, pos, ref, alt, array_agg(CAST(ROW(sample_id, \u0026#34;gt.alleles\u0026#34;) AS ROW(id VARCHAR, gts ARRAY(INTEGER)))) as samples, chrom from var_part_by_chrom group by 1,2,3,4,6 \rReplace the value for external location S3 path to a location in your own account.\n\r"
},
{
	"uri": "/en/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]